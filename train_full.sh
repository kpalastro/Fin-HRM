#!/bin/bash
# Full-scale HRM training with all original parameters
# Uses the complete configuration matching the PyTorch implementation

echo "ğŸš€ HRM Full-Scale Training"
echo "=========================="
echo "ğŸ”§ Full configuration:"
echo "   ğŸ“ Model: 512d, 2Ã—2 cycles, 4+4 layers (17.8M params)"
echo "   ğŸ“Š Data: 1000 training, 200 validation samples"
echo "   â±ï¸  Schedule: 2000 warmup steps, cosine decay"
echo "   ğŸ¯ Target: 20000 epochs, halt_max_steps=8"
echo "=========================="
echo ""

python train_yaml.py \
    --config config/cfg_pretrain.yaml \
    --halt_max_steps 8 \
    --train_samples 1000 \
    --val_samples 200 \
    --min_difficulty 20 \
    "$@"

echo ""
echo "ğŸ¯ Full-scale training completed!"