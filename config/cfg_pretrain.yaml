# HRM Training Configuration
# Based on original PyTorch HRM configuration

# Core hyperparameters
learning_rate: 0.0001
embedding_lr: 0.01  # Separate learning rate for embeddings (original uses 1e-2)
weight_decay: 0.1
embedding_weight_decay: 0.1
batch_size: 32  # Adjusted for MLX memory constraints (original: 768)
max_epochs: 2000

# Learning rate scheduling
warmup_steps: 200
min_lr_ratio: 0.1

# AdamATan2 parameters
beta1: 0.9
beta2: 0.95

# Model architecture
d_model: 512
H_cycles: 2
L_cycles: 2
H_layers: 4
L_layers: 4
expansion: 2.0

# ACT and Q-learning
halt_max_steps: 16
halt_exploration_prob: 0.1

# Data
train_samples: 1000
val_samples: 200
min_difficulty: 20
data_path: "data"

# Training
eval_interval: 200
save_every: 200
checkpoint_dir: "checkpoints"

# Monitoring
project_name: "hrm-mlx"
run_name: null  # Will be auto-generated if null